{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cbbbf379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications.vgg16 import VGG16,preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical,plot_model\n",
    "from tensorflow.keras.layers import Input,Dense,LSTM,Embedding,Dropout,add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "42feca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40455\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_id_caption = pd.read_csv(\"captions.txt\", sep=',')\n",
    "print(len(img_id_caption))\n",
    "display(img_id_caption.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "92ce2d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import wrap\n",
    "def readImage(path,img_size=224):\n",
    "    img = load_img(path,color_mode='rgb',target_size=(img_size,img_size))\n",
    "    img = img_to_array(img)\n",
    "    img = img/255.\n",
    "    \n",
    "    return img\n",
    "\n",
    "def display_images(temp_df):\n",
    "    temp_df = temp_df.reset_index(drop=True)\n",
    "    plt.figure(figsize = (20 , 20))\n",
    "    n = 0\n",
    "    for i in range(15):\n",
    "        n+=1\n",
    "        plt.subplot(5 , 5, n)\n",
    "        plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n",
    "        image = readImage(f\"Images/{temp_df.image[i]}\")\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa112d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(img_id_caption.sample(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a19d4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def process(data):\n",
    "    clean_data=[]\n",
    "    for word in data.split():\n",
    "        if len(word)>1:\n",
    "            word=word.lower()\n",
    "            word=re.sub('[^A-za-z]','',word)\n",
    "            clean_data.append(word)\n",
    "            \n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "22d4913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id_caption['cleaned_caption'] = img_id_caption['caption'].apply(lambda x: 'start '+' '.join(process(x))+ ' end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4e321918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>cleaned_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>1322323208_c7ecb742c6.jpg</td>\n",
       "      <td>Two young children playing with sticks at the ...</td>\n",
       "      <td>start two young children playing with sticks a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6726</th>\n",
       "      <td>2257798999_d9d1b9a45a.jpg</td>\n",
       "      <td>Orange , brown and white and brown dogs on gra...</td>\n",
       "      <td>start orange brown and white and brown dogs on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>1501297480_8db52c15b0.jpg</td>\n",
       "      <td>A child wearing shorts is moving a window scre...</td>\n",
       "      <td>start child wearing shorts is moving window sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30228</th>\n",
       "      <td>3518675890_2f65e23ff9.jpg</td>\n",
       "      <td>The brown dog is running on the grass .</td>\n",
       "      <td>start the brown dog is running on the grass end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18949</th>\n",
       "      <td>3040033126_9f4b88261b.jpg</td>\n",
       "      <td>The dog is leaping into the water .</td>\n",
       "      <td>start the dog is leaping into the water end</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "1469   1322323208_c7ecb742c6.jpg   \n",
       "6726   2257798999_d9d1b9a45a.jpg   \n",
       "2751   1501297480_8db52c15b0.jpg   \n",
       "30228  3518675890_2f65e23ff9.jpg   \n",
       "18949  3040033126_9f4b88261b.jpg   \n",
       "\n",
       "                                                 caption  \\\n",
       "1469   Two young children playing with sticks at the ...   \n",
       "6726   Orange , brown and white and brown dogs on gra...   \n",
       "2751   A child wearing shorts is moving a window scre...   \n",
       "30228            The brown dog is running on the grass .   \n",
       "18949                The dog is leaping into the water .   \n",
       "\n",
       "                                         cleaned_caption  \n",
       "1469   start two young children playing with sticks a...  \n",
       "6726   start orange brown and white and brown dogs on...  \n",
       "2751   start child wearing shorts is moving window sc...  \n",
       "30228    start the brown dog is running on the grass end  \n",
       "18949        start the dog is leaping into the water end  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_id_caption.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d7d30449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40455"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_captions = img_id_caption['cleaned_caption'].to_list()\n",
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "915e3684",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "img_id_caption.drop(columns=['caption'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f4f282d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "caption_dict = {k: v.tolist() for k, v in img_id_caption.groupby('image')['cleaned_caption']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8f147078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_id = list(caption_dict.keys())\n",
    "len(image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ab388de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "voc_size = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "97a4d413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 41, 3, 89, 169, 6, 118, 52, 394, 11, 391, 3, 27, 5193, 691, 1]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([all_captions[0]])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e6017819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(tokenizer,open(\"token.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7d7fd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_caption_len = max([len(i.split()) for i in all_captions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8cda46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7281, 810)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = image_id[:int(len(image_id)*.90)]\n",
    "val_data = image_id[int(len(image_id)*.90):]\n",
    "len(train_data),len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b883ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = img_id_caption[img_id_caption['image'].isin(train_data)]\n",
    "test_df = img_id_caption[img_id_caption['image'].isin(val_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fa92df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A local file was found, but it seems to be incomplete or outdated because the auto file hash does not match the original value of 64373286793e3c8b2b4e3219cbf3544b so we will re-download the data.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467096/553467096 [==============================] - 250s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = VGG16()\n",
    "fe = Model(inputs=model.input, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfa80dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134260544 (512.16 MB)\n",
      "Trainable params: 134260544 (512.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bea651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {}\n",
    "for img_name in tqdm(os.listdir('Images')):\n",
    "    img_path = os.path.join('Images',img_name)\n",
    "    img = load_img(img_path,target_size=(224,224))\n",
    "    img = img_to_array(img)\n",
    "    img = img.reshape((1,img.shape[0],img.shape[1],img.shape[2]))\n",
    "    img = preprocess_input(img)\n",
    "    feature = fe.predict(img, verbose=0)\n",
    "    features[img_name] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbae4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "features_test_print=pickle.load(open(\"features.pkl\",\"rb\"))\n",
    "features_test_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe5a063e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4096)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test_print[\"1000268201_693b08cb0e.jpg\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "766086bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data,caption_dict,features,tokenizer,max_length,voc_size,batch_size):\n",
    "    x1=[]\n",
    "    x2=[]\n",
    "    y=[]\n",
    "#     batch_size=64\n",
    "    n=0\n",
    "    while(1):\n",
    "        for id in data:\n",
    "            n+=1\n",
    "            captions = caption_dict[id]\n",
    "            for caption in captions:\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                for i in range(1,len(seq)):\n",
    "                    in_seq = seq[:i]\n",
    "                    out_seq = seq[i]\n",
    "                    in_seq = pad_sequences([in_seq],maxlen=max_caption_len)[0]\n",
    "                    out_seq = to_categorical([out_seq],num_classes=voc_size)[0]\n",
    "                    x1.append(features[id][0])\n",
    "                    x2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "            if n == batch_size:\n",
    "                x1,x2,y = np.array(x1),np.array(x2),np.array(y)\n",
    "                yield [x1,x2],y\n",
    "                x1,x2,y=list(),list(),list()\n",
    "                n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d82c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = data_generator(train_data,caption_dict,features,tokenizer,max_caption_len,voc_size,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "196c3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.4)(input1)\n",
    "fe2 = Dense(256,activation='relu')(fe1)\n",
    "\n",
    "input2 = Input(shape=(max_caption_len))\n",
    "se1 = Embedding(voc_size,256,mask_zero=True)(input2)\n",
    "se2 = Dropout(0.4)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "decoder1 = add([fe2,se3])\n",
    "decoder2 = Dense(256,activation='relu')(decoder1)\n",
    "\n",
    "output = Dense(voc_size,activation='softmax')(decoder2)\n",
    "\n",
    "model=Model(inputs=[input1,input2],outputs=output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e416a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\tumma\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "227/227 [==============================] - 485s 2s/step - loss: 5.2195\n",
      "227/227 [==============================] - 496s 2s/step - loss: 4.0049\n",
      "227/227 [==============================] - 495s 2s/step - loss: 3.5729\n",
      "227/227 [==============================] - 494s 2s/step - loss: 3.3084\n",
      "227/227 [==============================] - 497s 2s/step - loss: 3.1107\n",
      "227/227 [==============================] - 499s 2s/step - loss: 2.9602\n",
      "227/227 [==============================] - 495s 2s/step - loss: 2.8414\n",
      "227/227 [==============================] - 494s 2s/step - loss: 2.7512\n",
      "227/227 [==============================] - 493s 2s/step - loss: 2.6694\n",
      "227/227 [==============================] - 496s 2s/step - loss: 2.5984\n",
      "227/227 [==============================] - 494s 2s/step - loss: 2.5335\n",
      "227/227 [==============================] - 599s 3s/step - loss: 2.4819\n",
      "227/227 [==============================] - 689s 3s/step - loss: 2.4351\n",
      "227/227 [==============================] - 591s 3s/step - loss: 2.3926\n",
      "227/227 [==============================] - 529s 2s/step - loss: 2.3535\n",
      "227/227 [==============================] - 588s 3s/step - loss: 2.3144\n",
      "227/227 [==============================] - 530s 2s/step - loss: 2.2780\n",
      "227/227 [==============================] - 526s 2s/step - loss: 2.2464\n",
      "227/227 [==============================] - 525s 2s/step - loss: 2.2111\n",
      "227/227 [==============================] - 523s 2s/step - loss: 2.1836\n"
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "batch_size=32\n",
    "steps=len(train_data)//batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    generators = data_generator(train_data,caption_dict,features,tokenizer,max_caption_len,voc_size,batch_size)\n",
    "    model.fit(generators,epochs=1,steps_per_epoch=steps,verbose=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b66a6bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tumma\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7095e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(integer,tokenizer):\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if index==integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d828fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model,image,tokenizer,max_caption_len):\n",
    "    in_text = 'start'\n",
    "    for i in range(max_caption_len):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence],max_caption_len)\n",
    "        yhat = model.predict([image,sequence],verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = idx_to_word(yhat,tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text+=' '+ word\n",
    "        if word=='end':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3c30bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db185f3725547c486349d07c6265d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/810 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU_Score_1--> 0.5353887701136492\n",
      "BLEU_Score_2--> 0.3095948543257408\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "actual,predicted = list(),list()\n",
    "\n",
    "for img_id in tqdm(val_data):\n",
    "    cations = caption_dict[img_id]\n",
    "    y_pred = prediction(model,features[img_id],tokenizer,max_caption_len)\n",
    "    actual_caption = [i.split() for i in cations]\n",
    "    actual.append(actual_caption)\n",
    "    predicted.append(y_pred.split())\n",
    "    \n",
    "    \n",
    "print('BLEU_Score_1-->',corpus_bleu(actual,predicted,weights=(1,0,0,0,0)))\n",
    "print('BLEU_Score_2-->',corpus_bleu(actual,predicted,weights=(0.5,0.5,0,0,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955e25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0e617f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def enter_image_for_caption_generate(image_name):\n",
    "    image_path = os.path.join('Images',image_name)\n",
    "    image = Image.open(image_path)\n",
    "    captions = caption_dict[image_name]\n",
    "    print(\"Actual_Captions--->\")\n",
    "    for i in captions:\n",
    "        print(i)\n",
    "    print('-'*50)\n",
    "    y_pred = prediction(model,features[image_name],tokenizer,max_caption_len)\n",
    "    print('predicted-->')\n",
    "    print(y_pred)\n",
    "    print('-'*50)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec8645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6bb94ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def externel_caption_generate(image_name):\n",
    "    img11 = Image.open(image_name)\n",
    "    small_img = img11.resize((300, 300))\n",
    "\n",
    "    display(small_img)\n",
    "    img_path = image_name\n",
    "    \n",
    "    \n",
    "    img = load_img(img_path,target_size=(224,224))\n",
    "    img = img_to_array(img)\n",
    "    img = img.reshape((1,img.shape[0],img.shape[1],img.shape[2]))\n",
    "    img = preprocess_input(img)\n",
    "    ext_feature = fe.predict(img, verbose=0)\n",
    "    \n",
    "    y_pred = prediction(model,ext_feature,tokenizer,max_caption_len)\n",
    "    print('predicted-->')\n",
    "    print(y_pred)\n",
    "    return y_pred[6:-4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fd68849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted-->\n",
      "start man with pink headband and pink headband with pink headband with pink eyes in michael jackson standing in front of an umbrella end\n"
     ]
    }
   ],
   "source": [
    "import pyttsx3\n",
    "\n",
    "def text_to_speech(text):\n",
    "    engine = pyttsx3.init()\n",
    "\n",
    "    rate = engine.getProperty('rate')\n",
    "    engine.setProperty('rate', rate - 50)  \n",
    "\n",
    "    voices = engine.getProperty('voices')\n",
    "    engine.setProperty('voice', voices[0].id) \n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "text_to_speech(externel_caption_generate(\"D:\\\\SEM6\\\\DL\\\\data\\\\flickr30k_images\\\\flickr30k_images\\\\29389675.jpg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a66d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006d19c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
